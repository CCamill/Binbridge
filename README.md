# Binbridge

<p align="center">
  <b>äºŒè¿›åˆ¶ä»£ç å‡½æ•°å‘½åçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹</b>
</p>

Binbridge æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨ä¸ºäºŒè¿›åˆ¶ä»£ç ä¸­çš„å‡½æ•°ç”Ÿæˆæœ‰æ„ä¹‰çš„åç§°ã€‚å®ƒç»“åˆäº†æ»‘åŠ¨çª—å£ç¼–ç ã€æ³¨æ„åŠ›æŸ¥è¯¢æ¡¥æ¥ï¼ˆQ-Formerï¼‰å’Œè’¸é¦å¼æ€ç»´é“¾æŠ€æœ¯ï¼Œåœ¨ä¿æŒé«˜æ•ˆæ¨ç†çš„åŒæ—¶å®ç°äº†ä¸“å®¶çº§çš„å‘½åè´¨é‡ã€‚

## ğŸŒŸ ç‰¹æ€§

- **æ»‘åŠ¨çª—å£ç¼–ç **ï¼šé€šè¿‡é‡å ç­–ç•¥å¤„ç†ä»»æ„é•¿åº¦çš„æ±‡ç¼–åºåˆ—
- **Q-Former å‹ç¼©**ï¼šå°†æ•°åƒè¡Œæ±‡ç¼–ä»£ç å‹ç¼©ä¸º 64 ä¸ªé«˜å¯†åº¦è¯­ä¹‰å‘é‡
- **è’¸é¦å¼æ€ç»´é“¾**ï¼šåˆ©ç”¨ GPT-4 ç”Ÿæˆçš„åˆ†æä½œä¸ºè®­ç»ƒä¿¡å·
- **QLoRA é«˜æ•ˆå¾®è°ƒ**ï¼š4-bit é‡åŒ– + ä½ç§©é€‚é…å™¨ï¼Œå¤§å¹…é™ä½æ˜¾å­˜éœ€æ±‚
- **ç«¯åˆ°ç«¯æ¨ç†**ï¼šè‡ªåŠ¨è¿‡æ»¤åˆ†æè¿‡ç¨‹ï¼Œç›´æ¥è¾“å‡ºå‡½æ•°å

## ğŸ“ é¡¹ç›®ç»“æ„

```
binbridge/
â”œâ”€â”€ config.py              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ train.py               # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ inference.py           # æ¨ç†è„šæœ¬
â”œâ”€â”€ utils.py               # å·¥å…·å‡½æ•°
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ encoder.py         # æ»‘åŠ¨çª—å£ç¼–ç å™¨
â”‚   â”œâ”€â”€ qformer.py         # Q-Former æ¨¡å—
â”‚   â””â”€â”€ binbridge.py       # ä¸»æ¨¡å‹
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py         # æ•°æ®é›†å¤„ç†
â””â”€â”€ scripts/
    â””â”€â”€ prepare_data.py    # æ•°æ®å‡†å¤‡è„šæœ¬
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/yourusername/binbridge.git
cd binbridge

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
.\venv\Scripts\activate   # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### å‡†å¤‡æ•°æ®

æ•°æ®æ ¼å¼ä¸º JSONï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ï¼š
```json
{
    "assembly": "push rbp\nmov rbp, rsp\n...",
    "function_name": "calculate_checksum",
    "arch": "x86_64",
    "opt": "O2",
    "analysis": "å¯é€‰çš„æ€ç»´é“¾åˆ†æ..."
}
```

ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼š
```bash
python -c "from data.dataset import generate_sample_data; generate_sample_data('./data/train.json', 1000)"
```

### è®­ç»ƒ

```bash
python train.py \
    --train_data ./data/train.json \
    --eval_data ./data/eval.json \
    --output_dir ./outputs \
    --epochs 10 \
    --batch_size 4 \
    --learning_rate 2e-4 \
    --use_cot
```

### æ¨ç†

```bash
# æ‰¹é‡æ¨ç†
python inference.py \
    --model_path ./outputs/best_model \
    --input ./data/test.json \
    --output ./results.json \
    --batch_size 8

# äº¤äº’æ¨¡å¼
python inference.py \
    --model_path ./outputs/best_model \
    --interactive
```

## ğŸ—ï¸ æ¶æ„è¯¦è§£

### 1. æ»‘åŠ¨çª—å£ç¼–ç å™¨

å¤„ç†é•¿æ±‡ç¼–åºåˆ—çš„æ ¸å¿ƒç»„ä»¶ï¼š

```
è¾“å…¥åºåˆ— (é•¿åº¦ L)
    â†“
åˆ‡åˆ†ä¸º K ä¸ªé‡å çª—å£
    â†“
æ¯ä¸ªçª—å£ç‹¬ç«‹ç¼–ç 
    â†“
é‡å åŒºåŸŸç‰¹å¾å¹³å‡
    â†“
å…¨å±€ç‰¹å¾çŸ©é˜µ H_global
```

çª—å£æ•°é‡è®¡ç®—ï¼š$K = \lceil (L-W)/S \rceil + 1$

### 2. Q-Former æ³¨æ„åŠ›æ¡¥æ¥

å°†å†—é•¿ç‰¹å¾å‹ç¼©ä¸ºå›ºå®šé•¿åº¦ï¼š

```
å¯å­¦ä¹ æŸ¥è¯¢ Q âˆˆ R^{64Ã—768}
    â†“
è‡ªæ³¨æ„åŠ› (æŸ¥è¯¢é—´äº¤äº’)
    â†“
äº¤å‰æ³¨æ„åŠ› (ä» H_global æå–ä¿¡æ¯)
    â†“
çº¿æ€§æŠ•å½± â†’ LLM ç»´åº¦
    â†“
è½¯æç¤ºç¬¦ P âˆˆ R^{64Ã—4096}
```

### 3. è’¸é¦å¼æ€ç»´é“¾

è®­ç»ƒæ—¶ä½¿ç”¨ GPT-4 ç”Ÿæˆçš„åˆ†æä½œä¸ºç›‘ç£ä¿¡å·ï¼š

```
æ•™å¸ˆæ¨¡å‹ (GPT-4 + æºä»£ç )
    â†“
ç”Ÿæˆä¸‰é˜¶æ®µåˆ†æï¼š
1. æ¶æ„ä¸çº¦å®šåˆ†æ
2. æ•°æ®æµä¸ç‰¹å¾æå–
3. åŠŸèƒ½å½’çº³
    â†“
å­¦ç”Ÿæ¨¡å‹ (Binbridge)
ä»…è¾“å…¥æ±‡ç¼–ä»£ç ï¼Œå­¦ä¹ ç”Ÿæˆç›¸ä¼¼åˆ†æ
```

### 4. æ··åˆæŸå¤±å‡½æ•°

$$\mathcal{L} = \lambda_{analysis} \cdot \mathcal{L}_{analysis} + \lambda_{name} \cdot \mathcal{L}_{name}$$

- $\mathcal{L}_{analysis}$: åˆ†æéƒ¨åˆ†çš„äº¤å‰ç†µæŸå¤±
- $\mathcal{L}_{name}$: å‡½æ•°åéƒ¨åˆ†çš„äº¤å‰ç†µæŸå¤±
- é»˜è®¤æƒé‡: $\lambda_{analysis}=0.3$, $\lambda_{name}=0.7$

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| ç²¾ç¡®åŒ¹é…ç‡ | 45.2% |
| Token å‡†ç¡®ç‡ | 78.6% |
| BLEU-4 | 0.423 |
| æ¨ç†é€Ÿåº¦ | ~50 samples/sec |

*æ³¨ï¼šä»¥ä¸Šä¸ºç¤ºä¾‹æ•°æ®ï¼Œå®é™…æ€§èƒ½å–å†³äºè®­ç»ƒæ•°æ®è´¨é‡å’Œè§„æ¨¡*

## âš™ï¸ é…ç½®è¯´æ˜

### ç¼–ç å™¨é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `max_seq_length` | 512 | é¢„è®­ç»ƒæ¨¡å‹æœ€å¤§é•¿åº¦ |
| `window_size` | 512 | æ»‘åŠ¨çª—å£å¤§å° |
| `stride` | 256 | æ»‘åŠ¨æ­¥é•¿ |
| `freeze_encoder` | True | æ˜¯å¦å†»ç»“ç¼–ç å™¨ |

### Q-Former é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `num_query_tokens` | 64 | æŸ¥è¯¢å‘é‡æ•°é‡ |
| `num_hidden_layers` | 6 | Transformer å±‚æ•° |
| `cross_attention_freq` | 2 | äº¤å‰æ³¨æ„åŠ›é¢‘ç‡ |

### è®­ç»ƒé…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `learning_rate` | 2e-4 | å­¦ä¹ ç‡ |
| `batch_size` | 4 | æ‰¹å¤§å° |
| `gradient_accumulation_steps` | 4 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| `qlora_r` | 64 | LoRA ç§© |

## ğŸ“ æ•°æ®é‡å»ºè®®

æ ¹æ®å®æˆ˜ç»éªŒï¼š

| é˜¶æ®µ | æ•°æ®é‡ | æ•ˆæœ |
|------|--------|------|
| éªŒè¯åŸå‹ (MVP) | 500-1,000 æ¡ | æ¨¡å‹æŒæ¡ XML æ ¼å¼ |
| å…·å¤‡å®æˆ˜èƒ½åŠ› | 3,000-10,000 æ¡ | æ€ç»´é“¾è’¸é¦æ•ˆæœæ˜¾ç° |
| ç”Ÿäº§çº§/SOTA | 50,000+ æ¡ | è¦†ç›–å†·é—¨æ¶æ„å’Œä¼˜åŒ–çº§åˆ« |

**ç­–ç•¥å»ºè®®**ï¼šå…ˆç”¨ GPT-4 ç”Ÿæˆ 1,000 æ¡é«˜è´¨é‡çš„"æ•™ç§‘ä¹¦çº§"åˆ†æï¼Œç”¨è¿™ 1k æ¡å»å¾®è°ƒï¼Œæ•ˆæœå¯èƒ½å·²ç»æƒŠäººåœ°å¥½äº†ã€‚

## ğŸ”§ å¸¸è§é—®é¢˜

**Q: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**

A: å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
- å‡å° `batch_size`
- å¢å¤§ `gradient_accumulation_steps`
- ç¡®ä¿å¯ç”¨äº† 4-bit é‡åŒ– (`load_in_4bit=True`)
- å‡å°‘ `num_query_tokens`

**Q: å¦‚ä½•å¤„ç†é x86 æ¶æ„ï¼Ÿ**

A: åœ¨è®­ç»ƒæ•°æ®ä¸­åŒ…å« ARMã€MIPS ç­‰æ¶æ„çš„æ ·æœ¬ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨å­¦ä¹ æ¶æ„ç‰¹å®šçš„æ¨¡å¼ã€‚

**Q: æ¨ç†æ—¶éœ€è¦ä¿ç•™åˆ†æè¿‡ç¨‹å—ï¼Ÿ**

A: ä¸éœ€è¦ã€‚æ¨ç†æ—¶åˆ†æè¿‡ç¨‹ä½œä¸º"æ€ç»´æš‚å­˜åŒº"å¯¹é€»è¾‘è¿è´¯æ€§è‡³å…³é‡è¦ï¼Œä½†æœ€ç»ˆåªè¿”å›å‡½æ•°åã€‚

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

- BLIP-2 æ¶æ„å¯å‘äº† Q-Former è®¾è®¡
- CLAP-ASM æä¾›äº†é¢„è®­ç»ƒçš„æ±‡ç¼–ç¼–ç å™¨
- Llama 3.2 ä½œä¸ºåŸºç¡€è¯­è¨€æ¨¡å‹

## ğŸ“š å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº† Binbridgeï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{binbridge2024,
  title={Binbridge: Multimodal LLM for Binary Code Function Naming},
  author={Your Name},
  year={2024}
}
```
