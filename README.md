# Binbridge

<p align="center">
  <b>äºŒè¿›åˆ¶ä»£ç å‡½æ•°å‘½åçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹</b>
</p>

Binbridge æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨ä¸ºäºŒè¿›åˆ¶ä»£ç ä¸­çš„å‡½æ•°ç”Ÿæˆæœ‰æ„ä¹‰çš„åç§°ã€‚å®ƒç»“åˆäº†æ»‘åŠ¨çª—å£ç¼–ç ã€æ³¨æ„åŠ›æŸ¥è¯¢æ¡¥æ¥ï¼ˆQ-Formerï¼‰å’Œè’¸é¦å¼æ€ç»´é“¾æŠ€æœ¯ï¼Œåœ¨ä¿æŒé«˜æ•ˆæ¨ç†çš„åŒæ—¶å®ç°äº†ä¸“å®¶çº§çš„å‘½åè´¨é‡ã€‚

## ğŸŒŸ ç‰¹æ€§

- **æ»‘åŠ¨çª—å£ç¼–ç **ï¼šé€šè¿‡é‡å ç­–ç•¥å¤„ç†ä»»æ„é•¿åº¦çš„æ±‡ç¼–åºåˆ—
- **Q-Former å‹ç¼©**ï¼šå°†æ•°åƒè¡Œæ±‡ç¼–ä»£ç å‹ç¼©ä¸º 64 ä¸ªé«˜å¯†åº¦è¯­ä¹‰å‘é‡
- **è’¸é¦å¼æ€ç»´é“¾**ï¼šåˆ©ç”¨ GPT-4 ç”Ÿæˆçš„åˆ†æä½œä¸ºè®­ç»ƒä¿¡å·
- **QLoRA é«˜æ•ˆå¾®è°ƒ**ï¼š4-bit é‡åŒ– + ä½ç§©é€‚é…å™¨ï¼Œå¤§å¹…é™ä½æ˜¾å­˜éœ€æ±‚
- **ç«¯åˆ°ç«¯æ¨ç†**ï¼šè‡ªåŠ¨è¿‡æ»¤åˆ†æè¿‡ç¨‹ï¼Œç›´æ¥è¾“å‡ºå‡½æ•°å

## ğŸ“ é¡¹ç›®ç»“æ„

```
binbridge/
â”œâ”€â”€ config.py              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ train.py               # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ inference.py           # æ¨ç†è„šæœ¬
â”œâ”€â”€ utils.py               # å·¥å…·å‡½æ•°
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ tmp/
â”‚   â”œâ”€â”€ CoT_prompt.md      # æç¤ºè¯æ¨¡æ¿
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ encoder.py         # æ»‘åŠ¨çª—å£ç¼–ç å™¨
â”‚   â”œâ”€â”€ qformer.py         # Q-Former æ¨¡å—
â”‚   â””â”€â”€ binbridge.py       # ä¸»æ¨¡å‹
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ train.csv
|   â””â”€â”€ eval.csv
â””â”€â”€ scripts/
    â””â”€â”€ generate_cot_from_csv.py    # æ•°æ®å‡†å¤‡è„šæœ¬
    â””â”€â”€ csv_dataset.py         # æ•°æ®é›†å¤„ç†
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/yourusername/binbridge.git
cd binbridge

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
.\venv\Scripts\activate   # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### å‡†å¤‡æ•°æ®

æ•°æ®æ ¼å¼ä¸º CSVï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ï¼š

ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼š
```bash
python -c "from data.dataset import generate_sample_data; generate_sample_data('./data/train.csv', 1000)"
```

### è®­ç»ƒ

```bash
python train.py \
    --train_data ./data/train.csv \
    --eval_data ./data/eval.csv \
    --output_dir ./outputs \
    --epochs 10 \
    --batch_size 4 \
    --learning_rate 2e-4 \
    --use_cot
```

### æ¨ç†

```bash
# æ‰¹é‡æ¨ç†
python inference.py \
    --model_path ./outputs/best_model \
    --input ./data/test.csv \
    --output ./results.csv \
    --batch_size 8

# äº¤äº’æ¨¡å¼
python inference.py \
    --model_path ./outputs/best_model \
    --interactive
```

## ğŸ—ï¸ æ¶æ„è¯¦è§£

### 1. æ»‘åŠ¨çª—å£ç¼–ç å™¨

å¤„ç†é•¿æ±‡ç¼–åºåˆ—çš„æ ¸å¿ƒç»„ä»¶ï¼š

```
è¾“å…¥åºåˆ— (é•¿åº¦ L)
    â†“
åˆ‡åˆ†ä¸º K ä¸ªé‡å çª—å£
    â†“
æ¯ä¸ªçª—å£ç‹¬ç«‹ç¼–ç 
    â†“
é‡å åŒºåŸŸç‰¹å¾å¹³å‡
    â†“
å…¨å±€ç‰¹å¾çŸ©é˜µ H_global
```

çª—å£æ•°é‡è®¡ç®—ï¼š$K = \lceil (L-W)/S \rceil + 1$

### 2. Q-Former æ³¨æ„åŠ›æ¡¥æ¥

å°†å†—é•¿ç‰¹å¾å‹ç¼©ä¸ºå›ºå®šé•¿åº¦ï¼š

```
å¯å­¦ä¹ æŸ¥è¯¢ Q âˆˆ R^{64Ã—768}
    â†“
è‡ªæ³¨æ„åŠ› (æŸ¥è¯¢é—´äº¤äº’)
    â†“
äº¤å‰æ³¨æ„åŠ› (ä» H_global æå–ä¿¡æ¯)
    â†“
çº¿æ€§æŠ•å½± â†’ LLM ç»´åº¦
    â†“
è½¯æç¤ºç¬¦ P âˆˆ R^{64Ã—4096}
```

### 3. è’¸é¦å¼æ€ç»´é“¾

è®­ç»ƒæ—¶ä½¿ç”¨ Qwen ç”Ÿæˆçš„åˆ†æä½œä¸ºç›‘ç£ä¿¡å·ï¼š

```
æ•™å¸ˆæ¨¡å‹ (Qwen + æºä»£ç )
    â†“
ç”Ÿæˆä¸‰é˜¶æ®µåˆ†æï¼š
1. æ¶æ„ä¸çº¦å®šåˆ†æ
2. æ•°æ®æµä¸ç‰¹å¾æå–
3. åŠŸèƒ½å½’çº³
    â†“
å­¦ç”Ÿæ¨¡å‹ (Binbridge)
ä»…è¾“å…¥æ±‡ç¼–ä»£ç ï¼Œå­¦ä¹ ç”Ÿæˆç›¸ä¼¼åˆ†æ
```

### 4. æ··åˆæŸå¤±å‡½æ•°

$$\mathcal{L} = \lambda_{analysis} \cdot \mathcal{L}_{analysis} + \lambda_{name} \cdot \mathcal{L}_{name}$$

- $\mathcal{L}_{analysis}$: åˆ†æéƒ¨åˆ†çš„äº¤å‰ç†µæŸå¤±
- $\mathcal{L}_{name}$: å‡½æ•°åéƒ¨åˆ†çš„äº¤å‰ç†µæŸå¤±
- é»˜è®¤æƒé‡: $\lambda_{analysis}=0.3$, $\lambda_{name}=0.7$

## âš™ï¸ é…ç½®è¯´æ˜

### ç¼–ç å™¨é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `max_seq_length` | 2048 | é¢„è®­ç»ƒæ¨¡å‹æœ€å¤§é•¿åº¦ |
| `window_size` | 2048 | æ»‘åŠ¨çª—å£å¤§å° |
| `stride` | 256 | æ»‘åŠ¨æ­¥é•¿ |
| `freeze_encoder` | True | æ˜¯å¦å†»ç»“ç¼–ç å™¨ |

### Q-Former é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `num_query_tokens` | 64 | æŸ¥è¯¢å‘é‡æ•°é‡ |
| `num_hidden_layers` | 6 | Transformer å±‚æ•° |
| `cross_attention_freq` | 2 | äº¤å‰æ³¨æ„åŠ›é¢‘ç‡ |

### è®­ç»ƒé…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `learning_rate` | 2e-4 | å­¦ä¹ ç‡ |
| `batch_size` | 4 | æ‰¹å¤§å° |
| `gradient_accumulation_steps` | 4 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| `qlora_r` | 64 | LoRA ç§© |

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

- BLIP-2 æ¶æ„å¯å‘äº† Q-Former è®¾è®¡
- CLAP-ASM æä¾›äº†é¢„è®­ç»ƒçš„æ±‡ç¼–ç¼–ç å™¨
- Llama 3.2 ä½œä¸ºåŸºç¡€è¯­è¨€æ¨¡å‹

## ğŸ“š å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº† Binbridgeï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{binbridge2026,
  title={Binbridge},
  author={Jiajia Sun},
  year={2026}
}
```
